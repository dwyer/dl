#!/usr/bin/env python
#-*-coding:utf-8-*-
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import argparse
import contextlib
import datetime
import os
import re
import sys
import urllib2
import urlparse

from bs4 import BeautifulSoup

# CONFIG
DEFAULT_SAVE_DIR = os.path.join(os.environ['HOME'], 'dlpy')
DEFAULT_FILTER = r'\.(gif|jpe?g|png|webm)$'
DEFAULT_RENAMING_MASK = '*flatcurl*/*name*.*ext*'

try:
    import html5lib
    HTML_PARSER = 'html5lib'
except ImportError:
    HTML_PARSER = 'html.parser'


### RENAMING MASK ###

NOW = datetime.datetime.now()


def mask_curl(url):
    url_parts = urlparse.urlparse(url)
    dirname, _ = os.path.split(url_parts.path)
    return url_parts.netloc + dirname


def mask_flatcurl(url):
    return mask_curl(url).replace('/', '-')


def mask_name(url):
    url_parts = urlparse.urlparse(url)
    _, name = os.path.split(url_parts.path)
    name, _ = os.path.splitext(name)
    return name


def mask_ext(url):
    url_parts = urlparse.urlparse(url)
    _, ext = os.path.splitext(url_parts.path)
    if ext.startswith('.'):
        ext = ext[1:]
    return ext


_mask_num_counter = 0
def mask_num(_):
    global _mask_num_counter
    _mask_num_counter += 1
    return str(_mask_num_counter)


RENAMING_MASKS = {
    'curl': mask_curl,
    'ext': mask_ext,
    'flatcurl': mask_flatcurl,
    'name': mask_name,
    'url': lambda url: urlparse.urlparse(url).netloc,
    'subdirs': lambda url: os.path.dirname(urlparse.urlparse(url).path),

    'y': lambda _: NOW.strftime('%y'),
    'm': lambda _: NOW.strftime('%m'),
    'd': lambda _: NOW.strftime('%d'),
    'hh': lambda _: NOW.strftime('%H'),
    'mm': lambda _: NOW.strftime('%M'),

    'text': lambda _: 'click here to download',
    'num': mask_num,
}


def maskparse(mask):
    if not mask:
        return []
    if '*' not in mask:
        return [mask]
    if mask.startswith('*'):
        i = mask[1:].find('*')
        if i < 0:
            raise Exception('invalid mask: %r' % mask)
        i += 1
        sub = mask[1:i]
        sub = RENAMING_MASKS[sub]
        res = [sub]
        res.extend(maskparse(mask[i+1:]))
        return res
    i = mask.find('*')
    res = [mask[:i]]
    res.extend(maskparse(mask[i:]))
    return res


def maskfunc(mask):
    parts = maskparse(mask)
    return lambda url: ''.join(c(url) if callable(c) else c for c in parts)

### END RENAMING MASK ###

USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'


def urlopen(url, user_agent=USER_AGENT):
    """Opens URL with spoofed user-agent"""
    request = urllib2.Request(url)
    request.add_header('User-Agent', user_agent)
    return contextlib.closing(urllib2.urlopen(request))


def download(url, path):
    with urlopen(url) as response:
        content = response.read()
    try:
        os.makedirs(os.path.dirname(path))
    except OSError:
        pass
    with open(path, 'wb') as fp:
        fp.write(content)


def get_urls(url):
    with urlopen(url) as response:
        content = response.read()
    soup = BeautifulSoup(content, HTML_PARSER)
    for link in soup.find_all('a'):
        try:
            rel_url = link['href']
        except KeyError:
            continue
        yield urlparse.urljoin(url, rel_url)


def dl(url, filter=None, renaming_mask=None, save_dir=None, verbose=False,
       dryrun=False):
    filter = filter or DEFAULT_FILTER
    renaming_mask = renaming_mask or DEFAULT_RENAMING_MASK
    renaming_func = maskfunc(renaming_mask)
    save_dir = save_dir or DEFAULT_SAVE_DIR
    file_filter_regex = re.compile(filter, re.IGNORECASE)
    for abs_url in get_urls(url):
        _, filename = os.path.split(urlparse.urlparse(abs_url).path)
        if file_filter_regex.search(filename) is None:
            if verbose:
                print('ignoring', abs_url, file=sys.stderr)
            continue
        name = renaming_func(abs_url)
        path = os.path.join(save_dir, name)
        if os.path.exists(path):
            print('skipping', abs_url, file=sys.stderr)
        else:
            print('saving', abs_url, file=sys.stderr)
            if not dryrun:
                download(abs_url, path)
        print(path)


def main():
    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument('url')
    arg_parser.add_argument('--filter', default=None)
    arg_parser.add_argument('--renaming_mask', default=None)
    arg_parser.add_argument('--save_dir', default=None)
    arg_parser.add_argument('--verbose', action='store_true')
    arg_parser.add_argument('--dryrun', action='store_true')
    args = vars(arg_parser.parse_args())
    dl(**args)


if __name__ == '__main__':
    main()
