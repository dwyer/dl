#!/usr/bin/env python
# -*-coding:utf-8-*-
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import argparse
import contextlib
import datetime
import multiprocessing
import os
import re
import sys
import threading

from bs4 import BeautifulSoup
from concurrent import futures

try:
    # python 2
    import urllib2 as urllib_error
    import urllib2 as urllib_request
    import urlparse as urllib_parse
except ImportError:
    # python 3
    import urllib.error as urllib_error
    import urllib.parse as urllib_parse
    import urllib.request as urllib_request
    basestring = str

# CONFIG

DEFAULT_SAVE_DIR = os.path.join(os.environ['HOME'], 'dlpy')
DEFAULT_FILTER = r'\.(gif|jpe?g|png|webm)$'
DEFAULT_RENAMING_MASK = '*flatcurl*/*name*.*ext*'
MAX_WORKERS = multiprocessing.cpu_count() * 2 + 1
BLOCK_SIZE = os.stat(DEFAULT_SAVE_DIR).st_blksize
HTTP_REGEX = re.compile(r'^https?://', re.IGNORECASE)

try:
    import lxml  # noqa: F401
    HTML_PARSER = 'lxml'
except ImportError:
    HTML_PARSER = 'html.parser'

# PRINT FUNC

_print = print
print_lock = threading.Lock()


def print(*args, **kwargs):
    with print_lock:
        _print(*args, **kwargs)


# RENAMING MASK #

RENAMING_MASK_DELIM = '*'
NOW = datetime.datetime.now()


def mask_curl(url):
    url_parts = urllib_parse.urlparse(url)
    dirname, _ = os.path.split(url_parts.path)
    return url_parts.netloc + dirname


def mask_flatcurl(url):
    return mask_curl(url).replace('/', '-')


def mask_name(url):
    url_parts = urllib_parse.urlparse(url)
    _, name = os.path.split(url_parts.path)
    name, _ = os.path.splitext(name)
    return name


def mask_ext(url):
    url_parts = urllib_parse.urlparse(url)
    _, ext = os.path.splitext(url_parts.path)
    if ext.startswith('.'):
        ext = ext[1:]
    return ext


_mask_num_counter = 0


def mask_num(_):
    global _mask_num_counter
    _mask_num_counter += 1
    return str(_mask_num_counter)


RENAMING_MASKS = {
    'curl': mask_curl,
    'ext': mask_ext,
    'flatcurl': mask_flatcurl,
    'name': mask_name,
    'url': lambda url: urllib_parse.urlparse(url).netloc,
    'subdirs': lambda url: os.path.dirname(urllib_parse.urlparse(url).path),
    'y': lambda _: NOW.strftime('%y'),
    'm': lambda _: NOW.strftime('%m'),
    'd': lambda _: NOW.strftime('%d'),
    'hh': lambda _: NOW.strftime('%H'),
    'mm': lambda _: NOW.strftime('%M'),
    'date': str(NOW.date()),
    'time': str(NOW.time()),
    'datetime': NOW,
    'datetime_iso': NOW.isoformat(),
    'text': lambda _: 'click here to download',
    'num': mask_num,
}


def maskparse(mask):
    if not mask:
        return []
    if RENAMING_MASK_DELIM not in mask:
        return [mask]
    if mask.startswith(RENAMING_MASK_DELIM):
        i = mask[1:].find(RENAMING_MASK_DELIM)
        if i < 0:
            raise Exception('invalid mask: %r' % mask)
        i += 1
        sub = mask[1:i]
        sub = RENAMING_MASKS[sub]
        res = [sub]
        res.extend(maskparse(mask[i + 1:]))
        return res
    i = mask.find(RENAMING_MASK_DELIM)
    res = [mask[:i]]
    res.extend(maskparse(mask[i:]))
    return res


def maskfunc(mask):
    parts = maskparse(mask)
    return lambda url: ''.join(c(url) if callable(c) else c for c in parts)


# END RENAMING MASK

USER_AGENT = ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) '
              'AppleWebKit/537.36 (KHTML, like Gecko) '
              'Chrome/52.0.2743.116 Safari/537.36')


def urlopen(url, user_agent=USER_AGENT):
    """Opens URL with spoofed user-agent"""
    request = urllib_request.Request(url)
    request.add_header('User-Agent', user_agent)
    return contextlib.closing(urllib_request.urlopen(request))


def download(url, path):
    try:
        with urlopen(url) as response:
            dirpath = os.path.dirname(path)
            if not os.path.isdir(dirpath):
                os.makedirs(dirpath)
            with open(path, 'wb') as fp:
                while True:
                    block = response.read(BLOCK_SIZE)
                    if not block:
                        break
                    fp.write(block)
        return path
    except (IOError, OSError, urllib_error.HTTPError) as e:
        print(e, file=sys.stderr)
        return None


def get_urls(url):
    try:
        with urlopen(url) as response:
            content = response.read()
    except urllib_error.URLError as e:
        print(e, file=sys.stderr)
        return
    soup = BeautifulSoup(content, HTML_PARSER)
    for link in soup.find_all('a'):
        try:
            rel_url = link['href']
        except KeyError:
            continue
        yield urllib_parse.urljoin(url, rel_url)


def dl(url,
       filter=None,
       prefix=None,
       renaming_mask=None,
       save_dir=None,
       verbose=False,
       dryrun=False,
       depth=0,
       ignore=None):
    if ignore is None:
        ignore = set()
    filter = filter or DEFAULT_FILTER
    if isinstance(filter, basestring):
        filter = re.compile(filter, re.IGNORECASE)
    renaming_mask = renaming_mask or DEFAULT_RENAMING_MASK
    renaming_func = maskfunc(renaming_mask)
    save_dir = save_dir or DEFAULT_SAVE_DIR
    with futures.ThreadPoolExecutor(MAX_WORKERS) as executor:
        for abs_url in get_urls(url):
            if abs_url in ignore or HTTP_REGEX.search(abs_url) is None:
                continue
            _, filename = os.path.split(urllib_parse.urlparse(abs_url).path)
            if filter.search(filename) is None:
                if depth > 0:
                    ignore.add(abs_url)
                    print('following', abs_url, file=sys.stderr)
                    executor.submit(
                        dl,
                        abs_url,
                        filter=filter,
                        prefix=prefix,
                        renaming_mask=renaming_mask,
                        save_dir=save_dir,
                        verbose=verbose,
                        dryrun=dryrun,
                        depth=depth - 1,
                        ignore=ignore)
                elif verbose:
                    print('ignoring', abs_url, file=sys.stderr)
                continue
            ignore.add(abs_url)
            name = renaming_func(abs_url)
            if prefix:
                name = os.path.join(prefix, name)
            path = os.path.join(save_dir, name)
            if os.path.exists(path):
                print('skipping', abs_url, file=sys.stderr)
                print(path)
            else:
                print('saving', abs_url, file=sys.stderr)
                if dryrun:
                    print(path)
                else:
                    future = executor.submit(download, abs_url, path)
                    future.add_done_callback(
                        lambda future: print(future.result()))


def main():
    global HTML_PARSER, MAX_WORKERS
    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument('url')
    arg_parser.add_argument('--depth', type=int, default=0)
    arg_parser.add_argument('--filter', default=None)
    arg_parser.add_argument('--prefix', default=None)
    arg_parser.add_argument('--renaming_mask', default=None)
    arg_parser.add_argument('--save_dir', default=None)
    arg_parser.add_argument('--parser', default=HTML_PARSER)
    arg_parser.add_argument('--workers', type=int, default=MAX_WORKERS)
    arg_parser.add_argument('--verbose', action='store_true')
    arg_parser.add_argument('--dryrun', action='store_true')
    args = vars(arg_parser.parse_args())
    HTML_PARSER = args.pop('parser')
    MAX_WORKERS = args.pop('workers')
    dl(**args)


if __name__ == '__main__':
    main()
